{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "96b7476a",
      "metadata": {
        "id": "96b7476a"
      },
      "source": [
        "# 10-714 Homework 4 Extension"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a89d459",
      "metadata": {
        "id": "5a89d459"
      },
      "source": [
        "This homework is an extension of homework 4, where you will be implementing the Transformer architecture. For this assignment, all the things you need to implement is in the file `python/needle/nn/nn_transformer.py`. Other things in the needle library remains the same. This homework extension is built on homework 4, so make sure to copy the solutions from homework 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c1a5c18f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1a5c18f",
        "outputId": "3ed2554b-d13e-4832-a888-534a4435fc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/10714\n",
            "fatal: destination path 'hw4_extra' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/10714/hw4_extra\n",
            "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
            "  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-rykbmevo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-rykbmevo\n",
            "  Resolved https://github.com/dlsys10714/mugrade.git to commit ac73f725eb2ce0e2c6a38fa540035ee970b8b873\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mugrade\n",
            "  Building wheel for mugrade (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mugrade: filename=mugrade-1.3-py3-none-any.whl size=3708 sha256=c186e72fdf78e7be730a6e3185ff84aa0bad985ee1b36df4436175d46bfade76\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gspa3t40/wheels/df/c7/14/2b747145fc762900af3ff05bd0c9192c506e70db3ef3890239\n",
            "Successfully built mugrade\n",
            "Installing collected packages: mugrade\n",
            "Successfully installed mugrade-1.3\n",
            "Collecting pybind11\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-3.0.1\n"
          ]
        }
      ],
      "source": [
        "# Code to set up the assignment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "!mkdir -p 10714\n",
        "%cd /content/drive/MyDrive/10714\n",
        "!git clone https://github.com/dlsyscourse/hw4_extra.git\n",
        "%cd /content/drive/MyDrive/10714/hw4_extra\n",
        "\n",
        "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
        "!pip3 install pybind11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6a1d9d3a",
      "metadata": {
        "id": "6a1d9d3a"
      },
      "outputs": [],
      "source": [
        "# REQUIRED FOR MUGRADE\n",
        "MY_API_KEY = \"Ppy4ROJ4cS9i0cIXvmUM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5c9fb467",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c9fb467",
        "outputId": "b6ef2f23-5c2c-46ab-9f4a-d81cc335fa3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found pybind11: /usr/local/lib/python3.12/dist-packages/pybind11/include (found version \"3.0.1\")\n",
            "-- Found cuda, building cuda backend\n",
            "Sun Nov 16 19:14:03 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "-- Autodetected CUDA architecture(s):  7.5\n",
            "-- Configuring done (6.4s)\n",
            "-- Generating done (8.9s)\n",
            "-- Build files have been written to: /content/drive/MyDrive/10714/hw4_extra/build\n",
            "make[1]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "make[2]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "[-25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
            "[  0%] \u001b[32m\u001b[1mLinking CXX shared module /content/drive/MyDrive/10714/hw4_extra/python/needle/backend_ndarray/ndarray_backend_cpu.cpython-312-x86_64-linux-gnu.so\u001b[0m\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "[  0%] Built target ndarray_backend_cpu\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "[ 25%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module /content/drive/MyDrive/10714/hw4_extra/python/needle/backend_ndarray/ndarray_backend_cuda.cpython-312-x86_64-linux-gnu.so\u001b[0m\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "[ 50%] Built target ndarray_backend_cuda\n",
            "make[2]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n",
            "make[1]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n"
          ]
        }
      ],
      "source": [
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "45349235",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45349235",
        "outputId": "672726dc-b5ec-43ae-cac1-d75a6c6a69a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=./python\n",
            "env: NEEDLE_BACKEND=nd\n"
          ]
        }
      ],
      "source": [
        "%set_env PYTHONPATH ./python\n",
        "%set_env NEEDLE_BACKEND nd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f54d7073",
      "metadata": {
        "id": "f54d7073"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c5945207",
      "metadata": {
        "id": "c5945207"
      },
      "outputs": [],
      "source": [
        "# Download the PTB dataset\n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "!mkdir -p './data/ptb'\n",
        "# Download Penn Treebank dataset\n",
        "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
        "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
        "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
        "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cea5c0a",
      "metadata": {
        "id": "1cea5c0a"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a2f639",
      "metadata": {
        "id": "68a2f639"
      },
      "source": [
        "In the previous homework you have implemented two sequence models, the Recurrent Neural Network, and Long Short-Term Memory. These models were once the state-of-the-art and default architecture choices on sequence modelling tasks, including language generation, until recently when the famous paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani et al. 2017) came out in 2017. Since then, Transformers, a model architecture introduced in the aforementioned paper, have become the standard and most performant class of model on language tasks.\n",
        "\n",
        "You will be implementing a Transformer in `python/needle/nn/nn_transformer.py`.\n",
        "\n",
        "Transformers are composed of three mains components that you will implement.\n",
        "1. A masked multi-head attention mechanism that adaptively focuses on different timesteps of a sequence.\n",
        "2. A residual block consisting of the attention layer followed by a two-layer neural network applied independently at each timestep.\n",
        "3. A Transformer model consisting of several stacked residual blocks (in this homework you will implement a decoder-only transformer).\n",
        "\n",
        "![model](https://miro.medium.com/v2/1*ZCFSvkKtppgew3cc7BIaug.png)\n",
        "\n",
        "The above is a photo of the Transformer architecture from Vaswani et al. 2017. The version of the transformer you will implement is nearly identical, but has layer normalization applied at the start of each residual block (referred to as a [prenorm variant](https://arxiv.org/abs/2002.04745) of the Transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f094ff30",
      "metadata": {
        "id": "f094ff30"
      },
      "source": [
        "## Part 1: Implementing the Multi-Head Attention Activation Layer\n",
        "\n",
        "In this subproblem, you will be implementing the `forward` function of a \"base\" attention activation layer `MultiHeadAttention` in `python/needle/nn/nn_transformer.py`. This activation layer will take in three inputs:\n",
        "<p style=\"text-align: center;\">multi-head queries $Q \\in R^\\mathcal{B \\times H \\times T \\times D}$, keys $K \\in R^\\mathcal{B \\times H \\times T \\times D}$, and values $V \\in R^\\mathcal{B \\times H \\times T \\times D}$</p>\n",
        "\n",
        "where $B$ is the batch size, $H$ is the number of attention heads, $T$ is the sequence length, and $D$ is the hidden dimension.\n",
        "\n",
        "The attention output $X \\in R^{B \\times H \\times T \\times D}$ is computed as follows:\n",
        "\n",
        "<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q K^T}{\\sqrt{D}}) V$</p>\n",
        "\n",
        "Note that the matrix multiplications above are batched. This functionality is not natively supported in needle yet, so we have provided a convenient function `matmul` for batched matrix multiplications in `MultiHeadAttention`. Your goal in this section is to return $X$ given the input queries, keys, and values.\n",
        "\n",
        "For auto-regressive Transformer, this attention should support causal masking using the function `self.create_causal_mask` we have provided. This is to make sure that the prediction of next token only depends on it's previous tokens. Specifically, causal masking is applying a mask before the softmax so that the softmax probability is computed over a masked matrix of $\\frac{Q K^T}{\\sqrt{D}}$.\n",
        "\n",
        "In addition, your implementation should apply dropout to the attention softmax $\\text{softmax}(\\frac{Q K^T}{\\sqrt{D}})$. You can use the `self.dropout` function of the `MultiHeadAttention` module.\n",
        "\n",
        "Importantly, this layer is only an activation function, and has no trainable variables (these come later).\n",
        "\n",
        "Once you have finished your implementation, test your code with the following test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "df7eeaa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df7eeaa9",
        "outputId": "d6473442-87e1-4901-cc46-151d3335b24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/hw4_extra\n",
            "plugins: anyio-4.11.0, typeguard-4.4.4, langsmith-0.4.42\n",
            "collected 112 items / 96 deselected / 16 selected                              \u001b[0m\n",
            "\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 31%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.0-False-64-31-5-4] ______________\u001b[0m\n",
            "\n",
            "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
            "dropout = 0.0, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cpu()\n",
            "dropout    = 0.0\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875895e2090>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
            "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875895e2090>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ... -1.43703675e+00\n",
            "    -1.53633821e+00 -5.07373631e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875895e2090>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "        a_shape    = (4, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        b_transpose_shape = (4, 5, 1, 64, 31)\n",
            "        broadcast_shape = [4, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875895e2090>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787589573140>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787589573140>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x78766f826420>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x78766f826420>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787589573140>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... .... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cpu())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.0-False-64-31-5-8] ______________\u001b[0m\n",
            "\n",
            "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
            "dropout = 0.0, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cpu()\n",
            "dropout    = 0.0\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588ff5550>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
            "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588ff5550>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ... -2.09799916e-01\n",
            "    -7.34400272e-01 -9.15048599e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588ff5550>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "        a_shape    = (8, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        b_transpose_shape = (8, 5, 1, 64, 31)\n",
            "        broadcast_shape = [8, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588ff5550>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588133a40>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588133a40>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875881307d0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875881307d0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588133a40>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... .... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cpu())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.0-True-64-31-5-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
            "dropout = 0.0, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dropout    = 0.0\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x78758967d700>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
            "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x78758967d700>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ... -1.43703675e+00\n",
            "    -1.53633821e+00 -5.07373631e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x78758967d700>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "        a_shape    = (4, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        b_transpose_shape = (4, 5, 1, 64, 31)\n",
            "        broadcast_shape = [4, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x78758967d700>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588effb60>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588effb60>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588eff1d0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588eff1d0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588effb60>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "out        = NDArray([[[[[ 8.20215940e-02  0.00000000e+00  2.31238174e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.0-True-64-31-5-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
            "dropout = 0.0, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dropout    = 0.0\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588e210a0>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
            "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588e210a0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ... -2.09799916e-01\n",
            "    -7.34400272e-01 -9.15048599e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588e210a0>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "        a_shape    = (8, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        b_transpose_shape = (8, 5, 1, 64, 31)\n",
            "        broadcast_shape = [8, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588e210a0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588133680>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588133680>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588132990>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588132990>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588133680>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... .... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cpu())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.1-False-64-31-5-4] ______________\u001b[0m\n",
            "\n",
            "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
            "dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cpu()\n",
            "dropout    = 0.1\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588158c20>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
            "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588158c20>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ... -1.43703675e+00\n",
            "    -1.53633821e+00 -5.07373631e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588158c20>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "        a_shape    = (4, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        b_transpose_shape = (4, 5, 1, 64, 31)\n",
            "        broadcast_shape = [4, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588158c20>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x78758815aae0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x78758815aae0>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588159ee0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588159ee0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x78758815aae0>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "out        = NDArray([[[[[ 2.31238174e+00  0.00000000e+00  7.04460418e+30 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.1-False-64-31-5-8] ______________\u001b[0m\n",
            "\n",
            "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
            "dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cpu()\n",
            "dropout    = 0.1\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875880155e0>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
            "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875880155e0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ... -2.09799916e-01\n",
            "    -7.34400272e-01 -9.15048599e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875880155e0>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "        a_shape    = (8, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        b_transpose_shape = (8, 5, 1, 64, 31)\n",
            "        broadcast_shape = [8, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875880155e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015100>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015100>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588014cb0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588014cb0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015100>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "out        = NDArray([[[[[ 7.03066885e-01 -1.27914354e-01 -2.18606353e-01 ...\n",
            "     -1.04077518e+00  9.08335090e-01  4.29267794e-01]...738861e-01  2.12415963e-01  8.03906977e-01 ...\n",
            "      0.00000000e+00  0.00000000e+00  0.00000000e+00]]]]], device=cpu())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.1-True-64-31-5-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
            "dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dropout    = 0.1\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588efd220>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
            "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588efd220>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ... -1.43703675e+00\n",
            "    -1.53633821e+00 -5.07373631e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588efd220>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "        a_shape    = (4, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        b_transpose_shape = (4, 5, 1, 64, 31)\n",
            "        broadcast_shape = [4, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588efd220>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588016930>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588016930>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875880160c0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875880160c0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588016930>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "out        = NDArray([[[[[ 2.31238174e+00  0.00000000e+00  2.86252747e+01 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.1-True-64-31-5-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
            "dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dropout    = 0.1\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875880157f0>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
            "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875880157f0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ... -2.09799916e-01\n",
            "    -7.34400272e-01 -9.15048599e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875880157f0>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "        a_shape    = (8, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        b_transpose_shape = (8, 5, 1, 64, 31)\n",
            "        broadcast_shape = [8, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875880157f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015a60>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015a60>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875880144a0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875880144a0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015a60>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f50>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cpu())\n",
            "out        = NDArray([[[[[ 0.7030669  -0.12791435 -0.21860635 ... -1.0407752   0.9083351\n",
            "      0.4292678 ]\n",
            "    [ 0.7030669  -0.1279... 0.\n",
            "      0.        ]\n",
            "    [ 0.          0.          0.         ...  0.          0.\n",
            "      0.        ]]]]], device=cpu())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588f46f30>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m_____________ test_attention_activation[cuda-0.0-False-64-31-5-4] ______________\u001b[0m\n",
            "\n",
            "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
            "dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dropout    = 0.0\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588017a40>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
            "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588017a40>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ... -1.43703675e+00\n",
            "    -1.53633821e+00 -5.07373631e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588017a40>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "        a_shape    = (4, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        b_transpose_shape = (4, 5, 1, 64, 31)\n",
            "        broadcast_shape = [4, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588017a40>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015130>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015130>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588017020>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588017020>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588015130>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... ... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cuda())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m_____________ test_attention_activation[cuda-0.0-False-64-31-5-8] ______________\u001b[0m\n",
            "\n",
            "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
            "dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dropout    = 0.0\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x78758815aea0>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
            "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x78758815aea0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ... -2.09799916e-01\n",
            "    -7.34400272e-01 -9.15048599e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x78758815aea0>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "        a_shape    = (8, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        b_transpose_shape = (8, 5, 1, 64, 31)\n",
            "        broadcast_shape = [8, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x78758815aea0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588159ee0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588159ee0>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x78758815a570>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x78758815a570>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588159ee0>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... ... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cuda())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cuda-0.0-True-64-31-5-4] ______________\u001b[0m\n",
            "\n",
            "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
            "dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dropout    = 0.0\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588b38620>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
            "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588b38620>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ... -1.43703675e+00\n",
            "    -1.53633821e+00 -5.07373631e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588b38620>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "        a_shape    = (4, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        b_transpose_shape = (4, 5, 1, 64, 31)\n",
            "        broadcast_shape = [4, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588b38620>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588b38d70>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588b38d70>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588b38e30>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588b38e30>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588b38d70>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... ... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cuda())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cuda-0.0-True-64-31-5-8] ______________\u001b[0m\n",
            "\n",
            "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
            "dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dropout    = 0.0\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875875f6ab0>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
            "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875875f6ab0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ... -2.09799916e-01\n",
            "    -7.34400272e-01 -9.15048599e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875875f6ab0>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "        a_shape    = (8, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        b_transpose_shape = (8, 5, 1, 64, 31)\n",
            "        broadcast_shape = [8, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875875f6ab0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x7875875f6a80>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x7875875f6a80>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875875f7560>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875875f7560>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x7875875f6a80>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... ... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cuda())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m_____________ test_attention_activation[cuda-0.1-False-64-31-5-4] ______________\u001b[0m\n",
            "\n",
            "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
            "dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dropout    = 0.1\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875875f7ef0>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
            "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875875f7ef0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ... -1.43703675e+00\n",
            "    -1.53633821e+00 -5.07373631e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875875f7ef0>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "        a_shape    = (4, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        b_transpose_shape = (4, 5, 1, 64, 31)\n",
            "        broadcast_shape = [4, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7875875f7ef0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x7875875f4530>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x7875875f4530>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875875f4e60>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7875875f4e60>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x7875875f4530>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... ... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cuda())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m_____________ test_attention_activation[cuda-0.1-False-64-31-5-8] ______________\u001b[0m\n",
            "\n",
            "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
            "dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dropout    = 0.1\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588b58710>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
            "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588b58710>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ... -2.09799916e-01\n",
            "    -7.34400272e-01 -9.15048599e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588b58710>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "        a_shape    = (8, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        b_transpose_shape = (8, 5, 1, 64, 31)\n",
            "        broadcast_shape = [8, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588b58710>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588b59910>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588b59910>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588b5b9b0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588b5b9b0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588b59910>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... ... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cuda())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cuda-0.1-True-64-31-5-4] ______________\u001b[0m\n",
            "\n",
            "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
            "dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dropout    = 0.1\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588c7b050>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
            "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588c7b050>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ... -1.43703675e+00\n",
            "    -1.53633821e+00 -5.07373631e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588c7b050>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...1e+00]\n",
            "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
            "     1.54555345e+00 -5.07373631e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "        a_shape    = (4, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        b_transpose_shape = (4, 5, 1, 64, 31)\n",
            "        broadcast_shape = [4, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787588c7b050>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588c7aae0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [-1.19667161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787588c7aae0>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588c79b50>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787588c79b50>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787588c7aae0>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...67161e+00 -1.37920952e+00 -1.35955904e-02 ...\n",
            "     -1.43703675e+00 -1.53633821e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... ... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cuda())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...07298e-01  9.85594168e-02  1.14688802e+00 ...\n",
            "     -6.52230859e-01  1.54555345e+00 -5.07373631e-01]]]]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[31m\u001b[1m______________ test_attention_activation[cuda-0.1-True-64-31-5-8] ______________\u001b[0m\n",
            "\n",
            "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
            "dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dropout    = 0.1\n",
            "inner_dim  = 64\n",
            "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787587e32ed0>\n",
            "num_heads  = 5\n",
            "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
            "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
            "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
            "      dtype=float32)\n",
            "queries_len = 31\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565...e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787587e32ed0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 31\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        k_dim      = 64\n",
            "        k_transpose = needle.Tensor([[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...  6.56564355e-01\n",
            "    -2.48716593e-01 -1.28524327e...5e+00]\n",
            "   [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ... -2.09799916e-01\n",
            "    -7.34400272e-01 -9.15048599e-01]]]])\n",
            "        keys_values_len = 31\n",
            "        num_head   = 5\n",
            "        probs      = None\n",
            "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        q_dim      = 64\n",
            "        queries_len = 31\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787587e32ed0>\n",
            "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
            "     8.51403356e-01  5.37231565e...2e-01]\n",
            "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
            "     2.24718475e+00 -9.15048599e-01]]]])\n",
            "        v_dim      = 64\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "        a_shape    = (8, 5, 31, 1, 64)\n",
            "        b_transpose = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        b_transpose_shape = (8, 5, 1, 64, 31)\n",
            "        broadcast_shape = [8, 5, 31, 64, 31]\n",
            "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x787587e32ed0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.2852432...00]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]])\n",
            "        self       = needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.3723156...01]\n",
            "    [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787587e32690>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.372315...0]\n",
            "    [ 1.91521764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseMul object at 0x787587e32690>\n",
            "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787587e32330>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x787587e32330>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "        b          = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.EWiseMul object at 0x787587e32690>\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "        other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "        self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "other = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0cb0>\n",
            "other      = NDArray([[[[[-8.04154854e-03 -2.30135962e-01  6.66354537e-01 ...\n",
            "      6.56564355e-01 -2.48716593e-01 -1.28524327e+00]...21764e+00  2.19435230e-01  5.03034472e-01 ...\n",
            "     -2.09799916e-01 -7.34400272e-01 -9.15048599e-01]]]]], device=cuda())\n",
            "out        = NDArray([[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... ... 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]], device=cuda())\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x787588eb0330>\n",
            "self       = NDArray([[[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ...\n",
            "     -9.72985864e-01  8.51403356e-01  5.37231565e-01]...86164e-01  1.68578935e+00  5.42450488e-01 ...\n",
            "      1.22951768e-01  2.24718475e+00 -9.15048599e-01]]]]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.0-False-64-31-5-4]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.0-False-64-31-5-8]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.0-True-64-31-5-4]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.0-True-64-31-5-8]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.1-False-64-31-5-4]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.1-False-64-31-5-8]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.1-True-64-31-5-4]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.1-True-64-31-5-8]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.0-False-64-31-5-4]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.0-False-64-31-5-8]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.0-True-64-31-5-4]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.0-True-64-31-5-8]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.1-False-64-31-5-4]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.1-False-64-31-5-8]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.1-True-64-31-5-4]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.1-True-64-31-5-8]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31m====================== \u001b[31m\u001b[1m16 failed\u001b[0m, \u001b[33m96 deselected\u001b[0m\u001b[31m in 15.35s\u001b[0m\u001b[31m ======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -l -v -k \"attention_activation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d19da8e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d19da8e2",
        "outputId": "aaaf84b2-3c9d-4789-cd95-80fded2cb256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submit\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/drive/MyDrive/10714/hw4_extra\n",
            "plugins: anyio-4.11.0, typeguard-4.4.4, langsmith-0.4.42\n",
            "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
            "collected 4 items / 3 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/hw4_extra/test_transformer.py \n",
            "Submitting attention_activation...\n",
            "\u001b[31mF\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m_________________________ submit_attention_activation __________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92msubmit_attention_activation\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m## Attention activation\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfor\u001b[39;49;00m (batch_size, num_heads, queries_len, inner_dim,\u001b[90m\u001b[39;49;00m\n",
            "                causal, dropout, device) \u001b[95min\u001b[39;49;00m itertools.product(\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m31\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m64\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [ndl.cpu(), ndl.cuda()]\u001b[90m\u001b[39;49;00m\n",
            "                ):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "            np.random.seed(\u001b[94m87745\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "            q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "                batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
            "                queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "            layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
            "                dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">           result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
            "                ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:238: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:124: in forward\n",
            "    \u001b[0mscores = \u001b[96mself\u001b[39;49;00m.matmul(q, k_transpose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:69: in matmul\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m (a * b_transpose).sum(\u001b[96mlen\u001b[39;49;00m(a.shape) - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:319: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:46: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a * b\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:532: in __mul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[[[[ 3.18430901e-01  1.07797706e+00  1.03108823e+00 ...\n",
            "      4.20772761e-01  3.02625477e-01  3.38121343e+00]...131503e-01  1.93935108e+00  1.46118784e+00 ...\n",
            "      1.63788237e-02  6.66049957e-01  2.81633997e+00]]]]], device=cpu())\n",
            "other = NDArray([[[[[ 3.18430901e-01  5.17588615e-01 -3.29483032e-01 ...\n",
            "     -7.35721231e-01 -1.02571881e+00  2.42298633e-01]...758427e-02 -2.65242934e-01  6.39795244e-01 ...\n",
            "     -7.74297655e-01 -8.14617932e-01  2.81633997e+00]]]]], device=cpu())\n",
            "ewise_func = <built-in method ewise_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x7c902861ef90>\n",
            "scalar_func = <built-in method scalar_mul of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x7c902861efb0>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        other: Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[96mfloat\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        ewise_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "        scalar_func: Callable[[Any, Any, Any], \u001b[94mNone\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "    ) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
            "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:512: AssertionError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1msubmit_attention_activation\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
            "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[31m in 2.60s\u001b[0m\u001b[31m ========================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"attention_activation\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e65aea6",
      "metadata": {
        "id": "0e65aea6"
      },
      "source": [
        "## Part 2 Implementing the Self-Attention Layer with trainable parameters\n",
        "\n",
        "In this subproblem, you will use the `MultiHeadAttention` class you just implemented, and wrap it in a subclass of `Module` called `AttentionLayer` in `python/needle/nn/nn_transformer.py`.\n",
        "\n",
        "This layer implements the self-attention with prenorm (when k, and v are None in the `self.forward` call) and cross-attention (when k and v are present in the `self.forward` call). We have provided skeleton code with the appropriate layer attributes defined. Your job is to write the forward pass of the `AttentionLayer`. Note that you are implementing multi-head attention, where the number of attention heads is given by the `self.num_head` attribute of the `AttentionLayer` class.\n",
        "\n",
        "Given inputs $Q \\in R^\\mathcal{B \\times T \\times D'}$, keys $K \\in R^\\mathcal{B \\times T \\times D'}$, and values $V \\in R^\\mathcal{B \\times T \\times D'}$ where $B$ is the batch size, $T$ is the sequence length, and $D'$ is the embedding dimension. This layer performs the following computation sequentially:\n",
        "\n",
        "(1) map queries, key, and values to heads.\n",
        "\n",
        "<p style=\"text-align: center;\">$Q' = \\text{LayerNorm}_q (Q) \\; W_q$</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$K' = \\text{LayerNorm}_k (K) \\; W_k$</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$V' = \\text{LayerNorm}_v (V) \\; W_v$</p>\n",
        "\n",
        "where $\\text{LayerNorm}_q , \\text{LayerNorm}_k, \\text{LayerNorm}_v $ are the prenorm `self.prenorm_q`, `self.prenorm_k` and `self.prenorm_v` respectively.\n",
        "\n",
        "(2) unravel heads from the channels axis.\n",
        "\n",
        "<p style=\"text-align: center;\">$Q' \\in R^{B \\times T \\times (HD)} \\to Q' \\in R^{B \\times H \\times T \\times D} $</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$K' \\in R^{B \\times T \\times (HD)} \\to K' \\in R^{B \\times H \\times T \\times D} $</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$V' \\in R^{B \\times T \\times (HD)} \\to V' \\in R^{B \\times H \\times T \\times D} $</p>\n",
        "\n",
        "where $H$ and $D$ are `self.num_head` and `self.head_dim` respectively.\n",
        "\n",
        "(3) compute the multi-head attention activation.\n",
        "\n",
        "<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q' (K')^T}{\\sqrt{D}}) V'$</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$X \\in R^{B \\times H \\times T \\times D} \\to X \\in R^{B \\times T \\times H \\times D} $</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$X \\in R^{B \\times T \\times H \\times D} \\to X \\in R^{B \\times T \\times (HD)}$</p>\n",
        "\n",
        "The last two steps do a transpose and then reshape to get the hidden states to be the correct shape.\n",
        "\n",
        "(4) project back to the input space of the layer with `self.out_projection`\n",
        "\n",
        "<p style=\"text-align: center;\">$X' = X \\; W_o$</p>\n",
        "\n",
        "Your goal in this part is to return $X$ in the `self.forward` call of `AttentionLayer`. For debugging, you may capture the `probs` variable returned by the inner `MultiHeadAttention` module and store it in an attribute such as `self.probs` of the attention layer.\n",
        "\n",
        "Once finished, you may test your layer with the following test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "44b2fe04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44b2fe04",
        "outputId": "17388897-53a5-4ad4-844b-5266535315ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "   -9.10320699e-01  2.06379384e-0...4131e-01]\n",
            "  [ 7.93164298e-02  3.49814832e-01  1.62794697e+00 ...  1.30049407e-01\n",
            "    1.38784778e+00  7.90922821e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc087770>\n",
            "        v          = needle.Tensor([[[ 0.46454597 -0.29647216 -0.00637594 ...  0.8831419   1.8853118\n",
            "   -0.61863774]\n",
            "  [-0.30892298  0.8405...441459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc086450>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc086450>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc084a70>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc084a70>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc084c20>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc084c20>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc084a70>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.0-True-32-8-27-11-8] ________________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.0, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
            "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
            "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0842f0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
            "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
            "          0.3303766 , -0.71756196],\n",
            "        [-1.2459...\n",
            "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
            "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...2569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0842f0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[ 4.64545965e-01 -2.96472162e-01 -6.37594005e-03 ...  8.83141875e-01\n",
            "    1.88531184e+00 -6.18637741e-0...1563e-01]\n",
            "  [-9.47857574e-02  2.70587523e-02 -1.71661545e-02 ... -1.40956357e-01\n",
            "   -1.46573985e+00 -9.60531652e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0842f0>\n",
            "        v          = needle.Tensor([[[-0.18071565 -0.93578744 -0.1328542  ...  0.21799618  0.3303766\n",
            "   -0.71756196]\n",
            "  [-1.2459449   1.2537...52569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...24445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc084ad0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc084ad0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc086f90>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc086f90>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc084050>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc084050>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc086f90>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-False-32-8-27-5-4] ________________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
            "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
            "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbd57e00>\n",
            "num_head   = 8\n",
            "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
            "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
            "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
            "      dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
            "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
            "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
            "      dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...-3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbd57e00>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[-0.17591418 -0.81691647 -0.38043165 -1.4896185  -0.3424145\n",
            "    0.5975326  -0.55117905 -0.86266565 -0....24  1.9776016   0.54573816\n",
            "   -0.12426752  1.3688309  -0.28665462  1.8718244   1.2692264\n",
            "    1.2295368  -0.991682  ]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbd57e00>\n",
            "        v          = needle.Tensor([[[-1.32029665e+00  3.13428760e-01  3.91371369e-01 -1.15919903e-01\n",
            "   -1.81199983e-01  1.58794081e+00 -1... -3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbd54ad0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbd54ad0>\n",
            "        x          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd55670>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd55670>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbd542c0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbd542c0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd55670>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cpu())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-False-32-8-27-5-8] ________________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
            "         -0.37128997,  0.06527077],\n",
            "        [-1.0428...\n",
            "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
            "          0.526678  ,  0.26005384]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc3fc740>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
            "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
            "         -0.7493343 , -0.15542848],\n",
            "        [ 2.5435...\n",
            "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
            "          0.08334691,  0.17072625]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...9508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc3fc740>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[-1.3202966   0.31342876  0.39137137 ... -0.05047274 -0.37128997\n",
            "    0.06527077]\n",
            "  [-1.0428005   0.483...03746759  1.3814261\n",
            "   -0.48717907]\n",
            "  [ 0.7475417  -0.12896587 -0.21562193 ... -0.4836035   0.526678\n",
            "    0.26005384]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc3fc740>\n",
            "        v          = needle.Tensor([[[-0.05705854 -1.8222909   0.37688625 ... -0.30623534 -0.7493343\n",
            "   -0.15542848]\n",
            "  [ 2.543547    2.1037...69508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...76533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc086ae0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc086ae0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0852e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0852e0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc086ba0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc086ba0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0852e0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cpu())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-False-32-8-27-11-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
            "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
            "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc087dd0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
            "          0.31795904,  0.14328356]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
            "          1.8853118 , -0.61863774],\n",
            "        [-0.3089...\n",
            "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
            "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...41459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc087dd0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[ 3.31167579e-01 -1.21368361e+00 -1.94302607e+00 ... -3.49218071e-01\n",
            "   -9.10320699e-01  2.06379384e-0...4131e-01]\n",
            "  [ 7.93164298e-02  3.49814832e-01  1.62794697e+00 ...  1.30049407e-01\n",
            "    1.38784778e+00  7.90922821e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc087dd0>\n",
            "        v          = needle.Tensor([[[ 0.46454597 -0.29647216 -0.00637594 ...  0.8831419   1.8853118\n",
            "   -0.61863774]\n",
            "  [-0.30892298  0.8405...441459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc084470>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc084470>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e6f90>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e6f90>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0e55e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0e55e0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e6f90>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-False-32-8-27-11-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
            "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
            "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc038950>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
            "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
            "          0.3303766 , -0.71756196],\n",
            "        [-1.2459...\n",
            "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
            "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...2569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc038950>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[ 4.64545965e-01 -2.96472162e-01 -6.37594005e-03 ...  8.83141875e-01\n",
            "    1.88531184e+00 -6.18637741e-0...1563e-01]\n",
            "  [-9.47857574e-02  2.70587523e-02 -1.71661545e-02 ... -1.40956357e-01\n",
            "   -1.46573985e+00 -9.60531652e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc038950>\n",
            "        v          = needle.Tensor([[[-0.18071565 -0.93578744 -0.1328542  ...  0.21799618  0.3303766\n",
            "   -0.71756196]\n",
            "  [-1.2459449   1.2537...52569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...24445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0389e0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0389e0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc038d40>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc038d40>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc03b9e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc03b9e0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc038d40>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m________________ test_attention_layer[cpu-0.1-True-32-8-27-5-4] ________________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
            "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
            "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04b110>\n",
            "num_head   = 8\n",
            "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
            "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
            "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
            "      dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
            "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
            "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
            "      dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...-3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04b110>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[-0.17591418 -0.81691647 -0.38043165 -1.4896185  -0.3424145\n",
            "    0.5975326  -0.55117905 -0.86266565 -0....24  1.9776016   0.54573816\n",
            "   -0.12426752  1.3688309  -0.28665462  1.8718244   1.2692264\n",
            "    1.2295368  -0.991682  ]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04b110>\n",
            "        v          = needle.Tensor([[[-1.32029665e+00  3.13428760e-01  3.91371369e-01 -1.15919903e-01\n",
            "   -1.81199983e-01  1.58794081e+00 -1... -3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc04aea0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc04aea0>\n",
            "        x          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04b770>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04b770>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc048fb0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc048fb0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04b770>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cpu())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m________________ test_attention_layer[cpu-0.1-True-32-8-27-5-8] ________________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
            "         -0.37128997,  0.06527077],\n",
            "        [-1.0428...\n",
            "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
            "          0.526678  ,  0.26005384]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc007320>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
            "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
            "         -0.7493343 , -0.15542848],\n",
            "        [ 2.5435...\n",
            "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
            "          0.08334691,  0.17072625]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...9508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc007320>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[-1.3202966   0.31342876  0.39137137 ... -0.05047274 -0.37128997\n",
            "    0.06527077]\n",
            "  [-1.0428005   0.483...03746759  1.3814261\n",
            "   -0.48717907]\n",
            "  [ 0.7475417  -0.12896587 -0.21562193 ... -0.4836035   0.526678\n",
            "    0.26005384]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc007320>\n",
            "        v          = needle.Tensor([[[-0.05705854 -1.8222909   0.37688625 ... -0.30623534 -0.7493343\n",
            "   -0.15542848]\n",
            "  [ 2.543547    2.1037...69508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...76533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc007110>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc007110>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc039c40>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc039c40>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc038710>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc038710>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc039c40>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cpu())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-True-32-8-27-11-4] ________________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
            "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
            "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbd55af0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
            "          0.31795904,  0.14328356]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
            "          1.8853118 , -0.61863774],\n",
            "        [-0.3089...\n",
            "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
            "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...41459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbd55af0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[ 3.31167579e-01 -1.21368361e+00 -1.94302607e+00 ... -3.49218071e-01\n",
            "   -9.10320699e-01  2.06379384e-0...4131e-01]\n",
            "  [ 7.93164298e-02  3.49814832e-01  1.62794697e+00 ...  1.30049407e-01\n",
            "    1.38784778e+00  7.90922821e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbd55af0>\n",
            "        v          = needle.Tensor([[[ 0.46454597 -0.29647216 -0.00637594 ...  0.8831419   1.8853118\n",
            "   -0.61863774]\n",
            "  [-0.30892298  0.8405...441459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbd543b0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbd543b0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd555e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd555e0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbd55430>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbd55430>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd555e0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...8871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-True-32-8-27-11-8] ________________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cpu()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
            "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
            "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0399a0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
            "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
            "          0.3303766 , -0.71756196],\n",
            "        [-1.2459...\n",
            "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
            "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...2569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0399a0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[ 4.64545965e-01 -2.96472162e-01 -6.37594005e-03 ...  8.83141875e-01\n",
            "    1.88531184e+00 -6.18637741e-0...1563e-01]\n",
            "  [-9.47857574e-02  2.70587523e-02 -1.71661545e-02 ... -1.40956357e-01\n",
            "   -1.46573985e+00 -9.60531652e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0399a0>\n",
            "        v          = needle.Tensor([[[-0.18071565 -0.93578744 -0.1328542  ...  0.21799618  0.3303766\n",
            "   -0.71756196]\n",
            "  [-1.2459449   1.2537...52569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...24445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc038200>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc038200>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e60c0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e60c0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0e7020>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0e7020>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e60c0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...9741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cpu())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-False-32-8-27-5-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
            "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
            "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e5130>\n",
            "num_head   = 8\n",
            "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
            "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
            "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
            "      dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
            "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
            "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
            "      dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...-3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e5130>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[-0.17591418 -0.81691647 -0.38043165 -1.4896185  -0.3424145\n",
            "    0.5975326  -0.55117905 -0.86266565 -0....24  1.9776016   0.54573816\n",
            "   -0.12426752  1.3688309  -0.28665462  1.8718244   1.2692264\n",
            "    1.2295368  -0.991682  ]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e5130>\n",
            "        v          = needle.Tensor([[[-1.32029665e+00  3.13428760e-01  3.91371369e-01 -1.15919903e-01\n",
            "   -1.81199983e-01  1.58794081e+00 -1... -3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0e5b50>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0e5b50>\n",
            "        x          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04b0e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04b0e0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc04b980>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc04b980>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04b0e0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-False-32-8-27-5-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
            "         -0.37128997,  0.06527077],\n",
            "        [-1.0428...\n",
            "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
            "          0.526678  ,  0.26005384]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0c9a60>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
            "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
            "         -0.7493343 , -0.15542848],\n",
            "        [ 2.5435...\n",
            "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
            "          0.08334691,  0.17072625]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...9508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0c9a60>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[-1.3202966   0.31342876  0.39137137 ... -0.05047274 -0.37128997\n",
            "    0.06527077]\n",
            "  [-1.0428005   0.483...03746759  1.3814261\n",
            "   -0.48717907]\n",
            "  [ 0.7475417  -0.12896587 -0.21562193 ... -0.4836035   0.526678\n",
            "    0.26005384]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0c9a60>\n",
            "        v          = needle.Tensor([[[-0.05705854 -1.8222909   0.37688625 ... -0.30623534 -0.7493343\n",
            "   -0.15542848]\n",
            "  [ 2.543547    2.1037...69508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...76533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0ca450>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0ca450>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0c8ec0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0c8ec0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0cbcb0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0cbcb0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0c8ec0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m______________ test_attention_layer[cuda-0.0-False-32-8-27-11-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
            "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
            "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc085c40>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
            "          0.31795904,  0.14328356]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
            "          1.8853118 , -0.61863774],\n",
            "        [-0.3089...\n",
            "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
            "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...41459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc085c40>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[ 3.31167579e-01 -1.21368361e+00 -1.94302607e+00 ... -3.49218071e-01\n",
            "   -9.10320699e-01  2.06379384e-0...4131e-01]\n",
            "  [ 7.93164298e-02  3.49814832e-01  1.62794697e+00 ...  1.30049407e-01\n",
            "    1.38784778e+00  7.90922821e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc085c40>\n",
            "        v          = needle.Tensor([[[ 0.46454597 -0.29647216 -0.00637594 ...  0.8831419   1.8853118\n",
            "   -0.61863774]\n",
            "  [-0.30892298  0.8405...441459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc084aa0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc084aa0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e7890>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e7890>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0e4ef0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0e4ef0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e7890>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m______________ test_attention_layer[cuda-0.0-False-32-8-27-11-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
            "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
            "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc087920>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
            "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
            "          0.3303766 , -0.71756196],\n",
            "        [-1.2459...\n",
            "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
            "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...2569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc087920>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[ 4.64545965e-01 -2.96472162e-01 -6.37594005e-03 ...  8.83141875e-01\n",
            "    1.88531184e+00 -6.18637741e-0...1563e-01]\n",
            "  [-9.47857574e-02  2.70587523e-02 -1.71661545e-02 ... -1.40956357e-01\n",
            "   -1.46573985e+00 -9.60531652e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc087920>\n",
            "        v          = needle.Tensor([[[-0.18071565 -0.93578744 -0.1328542  ...  0.21799618  0.3303766\n",
            "   -0.71756196]\n",
            "  [-1.2459449   1.2537...52569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...24445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0c8950>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0c8950>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0c92b0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0c92b0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0cb2f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0cb2f0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0c92b0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-True-32-8-27-5-4] ________________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
            "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
            "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04f3e0>\n",
            "num_head   = 8\n",
            "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
            "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
            "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
            "      dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
            "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
            "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
            "      dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...-3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04f3e0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[-0.17591418 -0.81691647 -0.38043165 -1.4896185  -0.3424145\n",
            "    0.5975326  -0.55117905 -0.86266565 -0....24  1.9776016   0.54573816\n",
            "   -0.12426752  1.3688309  -0.28665462  1.8718244   1.2692264\n",
            "    1.2295368  -0.991682  ]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04f3e0>\n",
            "        v          = needle.Tensor([[[-1.32029665e+00  3.13428760e-01  3.91371369e-01 -1.15919903e-01\n",
            "   -1.81199983e-01  1.58794081e+00 -1... -3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc04e960>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc04e960>\n",
            "        x          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04faa0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04faa0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc04dd30>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc04dd30>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04faa0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-True-32-8-27-5-8] ________________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
            "         -0.37128997,  0.06527077],\n",
            "        [-1.0428...\n",
            "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
            "          0.526678  ,  0.26005384]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbe6d490>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
            "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
            "         -0.7493343 , -0.15542848],\n",
            "        [ 2.5435...\n",
            "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
            "          0.08334691,  0.17072625]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...9508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbe6d490>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[-1.3202966   0.31342876  0.39137137 ... -0.05047274 -0.37128997\n",
            "    0.06527077]\n",
            "  [-1.0428005   0.483...03746759  1.3814261\n",
            "   -0.48717907]\n",
            "  [ 0.7475417  -0.12896587 -0.21562193 ... -0.4836035   0.526678\n",
            "    0.26005384]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbe6d490>\n",
            "        v          = needle.Tensor([[[-0.05705854 -1.8222909   0.37688625 ... -0.30623534 -0.7493343\n",
            "   -0.15542848]\n",
            "  [ 2.543547    2.1037...69508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...76533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbe6c3e0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbe6c3e0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6cc80>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6cc80>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbe6d310>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbe6d310>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6cc80>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-True-32-8-27-11-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
            "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
            "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0ca900>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
            "          0.31795904,  0.14328356]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
            "          1.8853118 , -0.61863774],\n",
            "        [-0.3089...\n",
            "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
            "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...41459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0ca900>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[ 3.31167579e-01 -1.21368361e+00 -1.94302607e+00 ... -3.49218071e-01\n",
            "   -9.10320699e-01  2.06379384e-0...4131e-01]\n",
            "  [ 7.93164298e-02  3.49814832e-01  1.62794697e+00 ...  1.30049407e-01\n",
            "    1.38784778e+00  7.90922821e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0ca900>\n",
            "        v          = needle.Tensor([[[ 0.46454597 -0.29647216 -0.00637594 ...  0.8831419   1.8853118\n",
            "   -0.61863774]\n",
            "  [-0.30892298  0.8405...441459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0cbbc0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0cbbc0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0ca150>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0ca150>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0c9340>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0c9340>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0ca150>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-True-32-8-27-11-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.0, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.0\n",
            "input_dim  = 27\n",
            "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
            "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
            "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbe6c9e0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
            "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
            "          0.3303766 , -0.71756196],\n",
            "        [-1.2459...\n",
            "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
            "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...2569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbe6c9e0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[ 4.64545965e-01 -2.96472162e-01 -6.37594005e-03 ...  8.83141875e-01\n",
            "    1.88531184e+00 -6.18637741e-0...1563e-01]\n",
            "  [-9.47857574e-02  2.70587523e-02 -1.71661545e-02 ... -1.40956357e-01\n",
            "   -1.46573985e+00 -9.60531652e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bbe6c9e0>\n",
            "        v          = needle.Tensor([[[-0.18071565 -0.93578744 -0.1328542  ...  0.21799618  0.3303766\n",
            "   -0.71756196]\n",
            "  [-1.2459449   1.2537...52569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...24445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbe6f470>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbe6f470>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6f710>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6f710>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbe6f230>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbe6f230>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6f710>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-False-32-8-27-5-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
            "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
            "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0c82f0>\n",
            "num_head   = 8\n",
            "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
            "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
            "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
            "      dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
            "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
            "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
            "      dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...-3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0c82f0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[-0.17591418 -0.81691647 -0.38043165 -1.4896185  -0.3424145\n",
            "    0.5975326  -0.55117905 -0.86266565 -0....24  1.9776016   0.54573816\n",
            "   -0.12426752  1.3688309  -0.28665462  1.8718244   1.2692264\n",
            "    1.2295368  -0.991682  ]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0c82f0>\n",
            "        v          = needle.Tensor([[[-1.32029665e+00  3.13428760e-01  3.91371369e-01 -1.15919903e-01\n",
            "   -1.81199983e-01  1.58794081e+00 -1... -3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0cbb00>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0cbb00>\n",
            "        x          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc068800>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc068800>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc06bd40>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc06bd40>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc068800>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-False-32-8-27-5-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
            "         -0.37128997,  0.06527077],\n",
            "        [-1.0428...\n",
            "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
            "          0.526678  ,  0.26005384]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0390a0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
            "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
            "         -0.7493343 , -0.15542848],\n",
            "        [ 2.5435...\n",
            "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
            "          0.08334691,  0.17072625]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...9508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0390a0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[-1.3202966   0.31342876  0.39137137 ... -0.05047274 -0.37128997\n",
            "    0.06527077]\n",
            "  [-1.0428005   0.483...03746759  1.3814261\n",
            "   -0.48717907]\n",
            "  [ 0.7475417  -0.12896587 -0.21562193 ... -0.4836035   0.526678\n",
            "    0.26005384]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0390a0>\n",
            "        v          = needle.Tensor([[[-0.05705854 -1.8222909   0.37688625 ... -0.30623534 -0.7493343\n",
            "   -0.15542848]\n",
            "  [ 2.543547    2.1037...69508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...76533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc03a750>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc03a750>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc03b0b0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc03b0b0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc03b4a0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc03b4a0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc03b0b0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m______________ test_attention_layer[cuda-0.1-False-32-8-27-11-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
            "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
            "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc038dd0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
            "          0.31795904,  0.14328356]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
            "          1.8853118 , -0.61863774],\n",
            "        [-0.3089...\n",
            "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
            "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...41459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc038dd0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[ 3.31167579e-01 -1.21368361e+00 -1.94302607e+00 ... -3.49218071e-01\n",
            "   -9.10320699e-01  2.06379384e-0...4131e-01]\n",
            "  [ 7.93164298e-02  3.49814832e-01  1.62794697e+00 ...  1.30049407e-01\n",
            "    1.38784778e+00  7.90922821e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc038dd0>\n",
            "        v          = needle.Tensor([[[ 0.46454597 -0.29647216 -0.00637594 ...  0.8831419   1.8853118\n",
            "   -0.61863774]\n",
            "  [-0.30892298  0.8405...441459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc2de990>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc2de990>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc06ac90>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc06ac90>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc06a840>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc06a840>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc06ac90>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m______________ test_attention_layer[cuda-0.1-False-32-8-27-11-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = False, dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = False\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
            "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
            "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04d010>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
            "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
            "          0.3303766 , -0.71756196],\n",
            "        [-1.2459...\n",
            "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
            "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...2569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04d010>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[ 4.64545965e-01 -2.96472162e-01 -6.37594005e-03 ...  8.83141875e-01\n",
            "    1.88531184e+00 -6.18637741e-0...1563e-01]\n",
            "  [-9.47857574e-02  2.70587523e-02 -1.71661545e-02 ... -1.40956357e-01\n",
            "   -1.46573985e+00 -9.60531652e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04d010>\n",
            "        v          = needle.Tensor([[[-0.18071565 -0.93578744 -0.1328542  ...  0.21799618  0.3303766\n",
            "   -0.71756196]\n",
            "  [-1.2459449   1.2537...52569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...24445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc04c6e0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc04c6e0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04f3b0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04f3b0>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc04d130>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc04d130>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04f3b0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-True-32-8-27-5-4] ________________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
            "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
            "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04aea0>\n",
            "num_head   = 8\n",
            "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
            "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
            "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
            "      dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
            "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
            "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
            "      dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...-3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04aea0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[-0.17591418 -0.81691647 -0.38043165 -1.4896185  -0.3424145\n",
            "    0.5975326  -0.55117905 -0.86266565 -0....24  1.9776016   0.54573816\n",
            "   -0.12426752  1.3688309  -0.28665462  1.8718244   1.2692264\n",
            "    1.2295368  -0.991682  ]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04aea0>\n",
            "        v          = needle.Tensor([[[-1.32029665e+00  3.13428760e-01  3.91371369e-01 -1.15919903e-01\n",
            "   -1.81199983e-01  1.58794081e+00 -1... -3.68384868e-01  1.02694228e-01  6.39540374e-01  1.90380514e-01\n",
            "   -7.48570561e-02  2.28314137e+00 -8.06154013e-01]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  ...2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc049fa0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc049fa0>\n",
            "        x          = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
            "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
            "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210...937e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04a600>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.112121...7e-01\n",
            "  -9.6326299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04a600>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc04b260>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc04b260>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc04a600>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-9.1862809e-03  1.5231143e-01  1.5650477e-02 -8.0575675e-02\n",
            "  -1.6993038e-02  1.8561857e-02  6.1121210e-02 -...26299e-02 -2.6746407e-02  5.7954702e-04 -9.8356776e-02\n",
            "  -3.6831655e-02  6.6482082e-02 -1.9902039e-02]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-True-32-8-27-5-8] ________________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
            "         -0.37128997,  0.06527077],\n",
            "        [-1.0428...\n",
            "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
            "          0.526678  ,  0.26005384]]], dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04adb0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
            "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
            "seq_len    = 5\n",
            "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
            "         -0.7493343 , -0.15542848],\n",
            "        [ 2.5435...\n",
            "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
            "          0.08334691,  0.17072625]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...9508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04adb0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 5\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[-1.3202966   0.31342876  0.39137137 ... -0.05047274 -0.37128997\n",
            "    0.06527077]\n",
            "  [-1.0428005   0.483...03746759  1.3814261\n",
            "   -0.48717907]\n",
            "  [ 0.7475417  -0.12896587 -0.21562193 ... -0.4836035   0.526678\n",
            "    0.26005384]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 5\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 5\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc04adb0>\n",
            "        v          = needle.Tensor([[[-0.05705854 -1.8222909   0.37688625 ... -0.30623534 -0.7493343\n",
            "   -0.15542848]\n",
            "  [ 2.543547    2.1037...69508   0.6028309\n",
            "   -1.5467863 ]\n",
            "  [-0.59417653 -0.05847015 -0.01491287 ...  0.67793965  0.08334691\n",
            "    0.17072625]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...76533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc04e390>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc04e390>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...6776533  0.4178168\n",
            "   -1.1135957 ]\n",
            "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
            "   -0.991682  ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1...\n",
            "   6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6f710>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6....  6.26969859e-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6f710>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbe6fef0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbe6fef0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbe6f710>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-9.18628089e-03  1.52311429e-01  1.56504773e-02 -8.05756748e-02\n",
            "  -1.69930384e-02  1.85618568e-02  6.1121210...-02  5.36289848e-02  3.94321419e-02  8.63345563e-02\n",
            "   2.12707266e-01  1.16953216e-01 -5.70597462e-02]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-True-32-8-27-11-4] _______________\u001b[0m\n",
            "\n",
            "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 4\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
            "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
            "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e5eb0>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
            "          0.31795904,  0.14328356]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
            "          1.8853118 , -0.61863774],\n",
            "        [-0.3089...\n",
            "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
            "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...41459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e5eb0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 4\n",
            "        k          = needle.Tensor([[[ 3.31167579e-01 -1.21368361e+00 -1.94302607e+00 ... -3.49218071e-01\n",
            "   -9.10320699e-01  2.06379384e-0...4131e-01]\n",
            "  [ 7.93164298e-02  3.49814832e-01  1.62794697e+00 ...  1.30049407e-01\n",
            "    1.38784778e+00  7.90922821e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e5eb0>\n",
            "        v          = needle.Tensor([[[ 0.46454597 -0.29647216 -0.00637594 ...  0.8831419   1.8853118\n",
            "   -0.61863774]\n",
            "  [-0.30892298  0.8405...441459  -0.3096104\n",
            "    1.7642561 ]\n",
            "  [-0.10503879 -1.4589338  -0.06638545 ... -0.8672699   0.8674379\n",
            "    1.4639174 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0e4530>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 4\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bc0e4530>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...63     0.43104777\n",
            "    0.93574804]\n",
            "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
            "    0.14328356]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...4323\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]])\n",
            "        shape      = (4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e6750>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....23\n",
            "  -0.01998871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e6750>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0e5cd0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bc0e5cd0>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bc0e6750>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (4, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...871 -0.16064207  0.14437044 -0.02304489 -0.08361477  0.10748737\n",
            "   0.15421318  0.03931448 -0.02734539]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-True-32-8-27-11-8] _______________\u001b[0m\n",
            "\n",
            "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
            "causal = True, dropout = 0.1, device = cuda()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       result = layer(\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "batch_size = 8\n",
            "causal     = True\n",
            "device     = cuda()\n",
            "dim_head   = 32\n",
            "dropout    = 0.1\n",
            "input_dim  = 27\n",
            "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
            "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
            "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
            "      dtype=float32)\n",
            "layer      = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e7500>\n",
            "num_head   = 8\n",
            "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
            "          1.2252008 , -1.2465482 ],\n",
            "        [ 1.8416...\n",
            "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
            "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
            "seq_len    = 11\n",
            "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
            "          0.3303766 , -0.71756196],\n",
            "        [-1.2459...\n",
            "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
            "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...2569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]]))\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e7500>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        _          = 11\n",
            "        batch_size = 8\n",
            "        k          = needle.Tensor([[[ 4.64545965e-01 -2.96472162e-01 -6.37594005e-03 ...  8.83141875e-01\n",
            "    1.88531184e+00 -6.18637741e-0...1563e-01]\n",
            "  [-9.47857574e-02  2.70587523e-02 -1.71661545e-02 ... -1.40956357e-01\n",
            "   -1.46573985e+00 -9.60531652e-01]]])\n",
            "        k_dim      = 27\n",
            "        keys_values_len = 11\n",
            "        q          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "        q_dim      = 27\n",
            "        queries_len = 11\n",
            "        result     = None\n",
            "        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7fe8bc0e7500>\n",
            "        v          = needle.Tensor([[[-0.18071565 -0.93578744 -0.1328542  ...  0.21799618  0.3303766\n",
            "   -0.71756196]\n",
            "  [-1.2459449   1.2537...52569   -0.3062441\n",
            "   -0.47076544]\n",
            "  [ 0.25303677  0.72302336  0.13070038 ... -0.46647197  1.0845269\n",
            "    0.7676009 ]]])\n",
            "        v_dim      = 27\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.652...24445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]]),)\n",
            "        kwargs     = {}\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbe6eea0>\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = 8\n",
            "        mean       = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7fe8bbe6eea0>\n",
            "        x          = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
            "   -1.2465482 ]\n",
            "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
            "    0.92202413]\n",
            "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
            "    0.7909228 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0...1106\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]])\n",
            "        shape      = (8, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd56960>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0....06\n",
            "  -0.05799741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd56960>\n",
            "        tensor     = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbd54230>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[ValueError('Reshape size mismatch: prod(old) != prod(new)') raised in repr()] Tensor object at 0x7fe8bbd54230>\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "        a          = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7fe8bbd56960>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "new_shape = (8, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "new_shape  = (8, 1)\n",
            "self       = NDArray([[-0.15101065  0.25816017  0.02119857 -0.14995223  0.02382865 -0.03994709\n",
            "   0.06268708 -0.05891123  0.0074386...741 -0.0295905  -0.1735652  -0.3336252  -0.03737989 -0.15188207\n",
            "  -0.26173717 -0.07700106  0.00195326]], device=cuda())\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-False-32-8-27-5-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-False-32-8-27-5-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-False-32-8-27-11-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-False-32-8-27-11-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-True-32-8-27-5-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-True-32-8-27-5-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-True-32-8-27-11-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-True-32-8-27-11-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-False-32-8-27-5-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-False-32-8-27-5-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-False-32-8-27-11-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-False-32-8-27-11-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-True-32-8-27-5-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-True-32-8-27-5-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-True-32-8-27-11-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-True-32-8-27-11-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-5-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-5-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-11-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-11-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-5-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-5-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-11-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-11-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-5-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-5-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-11-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-11-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-5-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-5-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-11-4]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-11-8]\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31m====================== \u001b[31m\u001b[1m32 failed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 8.65s\u001b[0m\u001b[31m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -l -v -k \"attention_layer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "20d0bfad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20d0bfad",
        "outputId": "f9374f2c-27fe-4159-f0da-06270e5945f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submit\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/drive/MyDrive/10714/hw4_extra\n",
            "plugins: anyio-4.11.0, typeguard-4.4.4, langsmith-0.4.42\n",
            "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
            "collected 4 items / 3 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/hw4_extra/test_transformer.py \n",
            "Submitting attention_layer...\n",
            "\u001b[31mF\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m____________________________ submit_attention_layer ____________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92msubmit_attention_layer\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m## Attention Layer\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfor\u001b[39;49;00m (batch_size, seq_len, input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "                causal, dropout, device) \u001b[95min\u001b[39;49;00m itertools.product(\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m27\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m8\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m32\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "                    [ndl.cpu(), ndl.cuda()]\u001b[90m\u001b[39;49;00m\n",
            "                ):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "            np.random.seed(\u001b[94m87745\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "            q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "                batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "            ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "            k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "                batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "            ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "            v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
            "                batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
            "            ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "            layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
            "                input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
            "                dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">           result = layer(\u001b[90m\u001b[39;49;00m\n",
            "                ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.Tensor(k, device=device),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.Tensor(v, device=device),\u001b[90m\u001b[39;49;00m\n",
            "            )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:281: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:249: in forward\n",
            "    \u001b[0mq_norm = \u001b[96mself\u001b[39;49;00m.prenorm_q(q)  \u001b[90m# (batch_size, queries_len, q_features)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:304: in forward\n",
            "    \u001b[0mmean = ops.reshape(mean, (batch_size, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:345: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:233: in compute\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m a.reshape(\u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = NDArray([[ 0.12088176  0.06935643  0.02179717  0.12824365  0.07900467  0.30115145\n",
            "  -0.10423296  0.11269746  0.0681295...99121 -0.12389598 -0.00149018 -0.01523823  0.07291935 -0.0935757\n",
            "  -0.00635529  0.03024114 -0.08072742]], device=cpu())\n",
            "new_shape = (4, 1)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape: \u001b[96mtuple\u001b[39;49;00m[\u001b[96mint\u001b[39;49;00m, ...]) -> \u001b[33m\"\u001b[39;49;00m\u001b[33mNDArray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
            "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
            "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
            "    \u001b[33m    the original array.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Raises:\u001b[39;49;00m\n",
            "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
            "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# raise NotImplementedError()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m prod(\u001b[96mself\u001b[39;49;00m.shape) != prod(new_shape):\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReshape size mismatch: prod(old) != prod(new)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           ValueError: Reshape size mismatch: prod(old) != prod(new)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:267: ValueError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1msubmit_attention_layer\u001b[0m - ValueError: Reshape size mismatch: prod(old) != prod(new)\n",
            "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[31m in 2.51s\u001b[0m\u001b[31m ========================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"attention_layer\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa8fb30",
      "metadata": {
        "id": "9fa8fb30"
      },
      "source": [
        "## Part 3 Implementing a prenorm residual Transformer Layer\n",
        "\n",
        "You now have all the parts necessary to build a full Transformer by this point. In this subproblem, you will assemble the attention layer with a feedforward network into a stackable residual block. We have provided starter code in the `TransformerLayer` class.\n",
        "\n",
        "You will need to define the necessary class attributes in the `self.__init__` call of the module `TransformerLayer`, and fill in the forward pass in `self.forward`. Your transformer layer should support dropout applied to $X'$ from the previous step before adding a residual connection. Implement the following pseudocode of the layer, properly handling the intermediate tensor shapes:\n",
        "\n",
        "x - current sequence of hidden states\n",
        "\n",
        "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Attention}(x))$</p>\n",
        "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Linear}_{2}(\\text{Dropout}(\\text{ReLU}(\\text{Linear}_{1}(\\text{LayerNorm1d}(x))))))$</p>\n",
        "\n",
        "For the MLP, there are two Linear layers $\\text{Linear}_{1}$ and $\\text{Linear}_{2}$:\n",
        "- $\\text{Linear}_{1}$: input shape `q_features`, output shape `hidden_size`\n",
        "- $\\text{Linear}_{2}$: input shape `hidden_size`, output shape `q_features`\n",
        "\n",
        "Once finished, run the following test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e0fd87",
      "metadata": {
        "id": "59e0fd87"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"transformer_layer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74a6ecb",
      "metadata": {
        "id": "b74a6ecb"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"transformer_layer\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e78953",
      "metadata": {
        "id": "e0e78953"
      },
      "source": [
        "## Part 4 Implementing the Transformer model\n",
        "\n",
        "In this subsection, you will compose the residual transformer layers you implemented in the previous part to build the full Transformer model. Fill in the code in the `Transformer` class by defining a set of `num_layers` `TransformerLayer` modules with the appropriat parameters passed in from the parent `Transformer` class. Then, implement the `self.forward` call of the `Transformer`.\n",
        "\n",
        "As is, your current Transformer layers are permutation-invariant, and cannot tell which position each token is in the sequence. To break this symmetry, you will add a positional embedding to your Transformer.\n",
        "\n",
        "The original Transformer paper uses sinusoidal positional embeddings, and then adds to the input embeddings before the first `TransformerLayer`. These work well, but a more common strategy in modern Transformers is to learn the positional embeddings.\n",
        "\n",
        "To do this, you should use `needle.nn.Embedding`. In your Transformer implementation, create a learnable positional encoding using `needle.nn.Embedding` from homework 4, with `num_embeddings` set as `sequence_len`. Given an input sequence, you should create a tensor that has the timestep id of each token in the sequence (timesteps have increasing value, representing the position of a token in time), and use it like a word id.\n",
        "\n",
        "Last, add the created positional encoding to the input token embeddings before your transformer layers.\n",
        "\n",
        "Once complete, submit the following test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5fb0a7",
      "metadata": {
        "id": "ec5fb0a7"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"transformer_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c897377",
      "metadata": {
        "id": "4c897377"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"transformer_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899683fc",
      "metadata": {
        "id": "899683fc"
      },
      "source": [
        "Now, you can train a Transformer language model on the Penn Treebank dataset:\n",
        "\n",
        "Note: make sure to initialize a transformer model in the class `LanguageModel` of `apps/models.py`; also for Transformers, the final linear head `self.linear` should take in input dimension `embedding_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d118e5db",
      "metadata": {
        "id": "d118e5db"
      },
      "outputs": [],
      "source": [
        "import needle as ndl\n",
        "sys.path.append('./apps')\n",
        "from models import LanguageModel\n",
        "from simple_ml import train_ptb, evaluate_ptb\n",
        "\n",
        "device = ndl.cuda()\n",
        "corpus = ndl.data.Corpus(\"data/ptb\")\n",
        "train_data = ndl.data.batchify(corpus.train, batch_size=256, device=device, dtype=\"float32\")\n",
        "model = LanguageModel(20, len(corpus.dictionary), hidden_size=32, num_layers=1, seq_model='transformer', seq_len=20, device=device)\n",
        "train_ptb(model, train_data, seq_len=20, n_epochs=10, device=device, lr=0.003, optimizer=ndl.optim.Adam)\n",
        "evaluate_ptb(model, train_data, seq_len=20, device=device)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}